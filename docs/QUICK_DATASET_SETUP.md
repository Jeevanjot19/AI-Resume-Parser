# Quick Dataset Setup - Visual Guide

## ðŸ“¥ Step-by-Step: Manual Download

### 1ï¸âƒ£ Visit Kaggle
```
ðŸŒ https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset
```

### 2ï¸âƒ£ Click Download Button
- You'll need a free Kaggle account (sign up if you don't have one)
- Click the blue "Download" button
- File will download as `archive.zip` or similar

### 3ï¸âƒ£ Extract the ZIP
```
ðŸ“¦ archive.zip
   â””â”€â”€ ðŸ“„ Resume.csv  â† This is what you need!
```

### 4ï¸âƒ£ Create Folder Structure
In your project directory, create this folder:

**Windows PowerShell:**
```powershell
New-Item -ItemType Directory -Force -Path "data\kaggle_resume_dataset"
```

**Windows Command Prompt:**
```cmd
mkdir data\kaggle_resume_dataset
```

**Linux/Mac:**
```bash
mkdir -p data/kaggle_resume_dataset
```

### 5ï¸âƒ£ Copy Resume.csv
Copy the extracted `Resume.csv` file to:
```
D:\gemini hackathon\resume_parser_ai\data\kaggle_resume_dataset\Resume.csv
```

Your folder structure should look like:
```
resume_parser_ai/
â”œâ”€â”€ app/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ kaggle_resume_dataset/
â”‚       â””â”€â”€ Resume.csv          â† File should be here
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ import_kaggle_dataset.py
â””â”€â”€ ...
```

### 6ï¸âƒ£ Run Import Script
```bash
python scripts/import_kaggle_dataset.py
```

### 7ï¸âƒ£ Wait for Processing
```
Processing resume 1/2400 - Category: Data Science
âœ“ Resume 0 processed successfully (ID: uuid-here)

Processing resume 2/2400 - Category: HR
âœ“ Resume 1 processed successfully (ID: uuid-here)

...
```

By default, it processes **50 resumes** for testing.

### 8ï¸âƒ£ Verify Import
The script automatically verifies at the end:
```
Total resumes in database: 50
Sample resumes:
- uuid-1: kaggle_resume_0.txt (Category: Data Science)
  Skills: ['Python', 'Machine Learning', 'TensorFlow', ...]
```

## âœ… Done!

Now you can:
- Search resumes via API
- Test job matching
- Analyze resume quality
- Use semantic search

## ðŸš€ Test It Out

```bash
# Search for Python developers
curl -X POST http://localhost:8000/api/v1/resumes/search \
  -H "Content-Type: application/json" \
  -d '{"query": "Python developer with machine learning", "top_k": 5}'
```

## ðŸ“Š Dataset Stats

The Resume.csv contains:
- **2,400+ resumes**
- **24 job categories**
- Categories include:
  - Data Science
  - Software Engineering (Java, Python, .NET, etc.)
  - DevOps Engineer
  - Database Administrator
  - HR
  - Business Analyst
  - Sales
  - And more!

## ðŸ’¡ Tips

1. **Processing Limit**: By default, script processes 50 resumes. To process more, edit line 104 in `scripts/import_kaggle_dataset.py`:
   ```python
   if processed_count >= 50:  # Change to 100, 500, or remove this check
   ```

2. **Check Progress**: Watch the terminal output to see each resume being processed

3. **Troubleshooting**: If import fails, check:
   - Resume.csv is in the correct folder
   - Database is running: `docker-compose ps`
   - Elasticsearch is running: `docker-compose ps`

4. **Re-import**: To re-import, you can delete existing data or use different IDs

## ðŸŽ¯ What Happens During Import?

Each resume goes through:
1. âœ… Text extraction
2. âœ… NER (Named Entity Recognition) - extracts names, emails, skills, etc.
3. âœ… Industry classification - determines industry fit
4. âœ… Career level detection - Entry, Junior, Mid, Senior, etc.
5. âœ… Skills extraction - technical and soft skills
6. âœ… Embedding generation - 768-dim vectors for semantic search
7. âœ… Quality analysis - AI-powered quality scoring
8. âœ… Storage in PostgreSQL
9. âœ… Indexing in Elasticsearch

All using **real AI models** - no mocks! ðŸ¤–
